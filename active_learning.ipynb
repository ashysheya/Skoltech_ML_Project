{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "\n",
    "from utils import load_sentences_and_labels, get_normalized_sentences, \\\n",
    "        load_word2vec, build_token_embeddings_tensor, encode_sentences_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = load_sentences_and_labels('factRuEval-2016-master/devset')\n",
    "\n",
    "#normalized_sentences = get_normalized_sentences(sentences)\n",
    "#with open('devset-normalized_sentences.pk', 'wb') as normalized_sentences_dump:\n",
    "#    pickle.dump(normalized_sentences, normalized_sentences_dump)\n",
    "\n",
    "with open('devset-normalized_sentences.pk', 'rb') as normalized_sentences_dump:\n",
    "    normalized_sentences = pickle.load(normalized_sentences_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(sentences)\n",
    "train_data_size = int(data_size*0.8)\n",
    "val_data_size = data_size - train_data_size\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "data_permutation = np.random.permutation(data_size)\n",
    "\n",
    "train_sentences = sentences[data_permutation[:train_data_size]]\n",
    "train_normalized_sentences = sentences[data_permutation[:train_data_size]]\n",
    "train_labels = labels[data_permutation[:train_data_size]]\n",
    "\n",
    "symbol_codes, token_codes, label_codes, sentence_dimension, token_dimension, \\\n",
    "        train_encoded_symbols, train_encoded_tokens, train_encoded_labels = \\\n",
    "        encode_sentences_and_labels(train_sentences, train_normalized_sentences,\n",
    "                                    train_labels)\n",
    "\n",
    "unique_symbols_count = len(symbol_codes)\n",
    "unique_tokens_count = len(token_codes)\n",
    "unique_labels_count = len(label_codes)\n",
    "\n",
    "val_sentences = sentences[data_permutation[train_data_size:]]\n",
    "val_normalized_sentences = sentences[data_permutation[train_data_size:]]\n",
    "val_labels = labels[data_permutation[train_data_size:]]\n",
    "\n",
    "val_encoded_symbols, val_encoded_tokens, val_encoded_labels = \\\n",
    "        encode_sentences_and_labels(val_sentences, val_normalized_sentences,\n",
    "        val_labels, codes=(symbol_codes, token_codes, label_codes),\n",
    "        dimensions=(sentence_dimension, token_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, unique_symbols_count, unique_labels_count, token_dimension,\n",
    "                 token_embeddings_tensor):\n",
    "\n",
    "        super(NERTagger, self).__init__()\n",
    "\n",
    "        #TODO: add residual connections\n",
    "\n",
    "        symbol_embedding_dimension = 64\n",
    "        symbol_convolution_out_channels = 128\n",
    "        symbol_convolution_kernel_size = 3\n",
    "        symbol_convolution_padding = 1\n",
    "\n",
    "        token_convolution_out_channels = 256\n",
    "        token_convolution_kernel_size = 5\n",
    "        token_convolution_padding = 2\n",
    "\n",
    "        self.symbol_embedding = nn.Embedding(unique_symbols_count,\n",
    "                                             symbol_embedding_dimension)\n",
    "        self.symbol_convolution = nn.Conv1d(symbol_embedding_dimension,\n",
    "                                            symbol_convolution_out_channels,\n",
    "                                            symbol_convolution_kernel_size,\n",
    "                                            padding=symbol_convolution_padding)\n",
    "        self.symbol_relu = nn.ReLU()\n",
    "        self.symbol_dropout = nn.Dropout()\n",
    "        self.symbol_batchnorm = nn.BatchNorm1d(symbol_convolution_out_channels)\n",
    "        self.symbol_pooling = nn.MaxPool1d(token_dimension,\n",
    "                                           stride=token_dimension)\n",
    "\n",
    "        unique_tokens_count, token_embedding_dimension = token_embeddings_tensor.shape\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(unique_tokens_count,\n",
    "                                            token_embedding_dimension)\n",
    "        self.token_embedding.weight = nn.Parameter(token_embeddings_tensor)\n",
    "\n",
    "        self.token_convolution1 = nn.Conv1d(token_embedding_dimension + \\\n",
    "                                            symbol_convolution_out_channels,\n",
    "                                            token_convolution_out_channels,\n",
    "                                            token_convolution_kernel_size,\n",
    "                                            padding=token_convolution_padding)\n",
    "        self.token_relu1 = nn.ReLU()\n",
    "        self.token_dropout1 = nn.Dropout()\n",
    "        self.token_batchnorm1 = nn.BatchNorm1d(token_convolution_out_channels)\n",
    "\n",
    "        self.token_convolution2 = nn.Conv1d(token_convolution_out_channels,\n",
    "                                            token_convolution_out_channels,\n",
    "                                            token_convolution_kernel_size,\n",
    "                                            padding=token_convolution_padding)\n",
    "        self.token_relu2 = nn.ReLU()\n",
    "        self.token_dropout2 = nn.Dropout()\n",
    "        self.token_batchnorm2 = nn.BatchNorm1d(token_convolution_out_channels)\n",
    "\n",
    "        self.lstm = nn.LSTM(token_convolution_out_channels, unique_labels_count)\n",
    "\n",
    "    def forward(self, encoded_symbols, encoded_tokens):\n",
    "\n",
    "        symbol_embeddings = self.symbol_embedding(encoded_symbols)\n",
    "        symbol_embeddings = torch.transpose(symbol_embeddings, 1, 2)\n",
    "        symbol_forwarded_data = self.symbol_convolution(symbol_embeddings)\n",
    "        symbol_forwarded_data = self.symbol_relu(symbol_forwarded_data)\n",
    "        symbol_forwarded_data = self.symbol_dropout(symbol_forwarded_data)\n",
    "        symbol_forwarded_data = self.symbol_batchnorm(symbol_forwarded_data)\n",
    "        symbol_forwarded_data = self.symbol_pooling(symbol_forwarded_data)\n",
    "        symbol_forwarded_data = torch.transpose(symbol_forwarded_data, 1, 2)\n",
    "\n",
    "        token_embeddings = self.token_embedding(encoded_tokens)\n",
    "        token_forwarded_data = torch.cat((symbol_forwarded_data, token_embeddings), dim=2)\n",
    "        token_forwarded_data = torch.transpose(token_forwarded_data, 1, 2)\n",
    "\n",
    "        token_forwarded_data = self.token_convolution1(token_forwarded_data)\n",
    "        token_forwarded_data = self.token_relu1(token_forwarded_data)\n",
    "        token_forwarded_data = self.token_dropout1(token_forwarded_data)\n",
    "        token_forwarded_data = self.token_batchnorm1(token_forwarded_data)\n",
    "\n",
    "        token_forwarded_data = self.token_convolution2(token_forwarded_data)\n",
    "        token_forwarded_data = self.token_relu2(token_forwarded_data)\n",
    "        token_forwarded_data = self.token_dropout2(token_forwarded_data)\n",
    "        token_forwarded_data = self.token_batchnorm2(token_forwarded_data)\n",
    "\n",
    "        result_forwarded_data = torch.transpose(token_forwarded_data, 1, 2)\n",
    "        result_forwarded_data = torch.transpose(result_forwarded_data, 0, 1)\n",
    "        result_forwarded_data = self.lstm(result_forwarded_data)[0]\n",
    "        result_forwarded_data = torch.transpose(result_forwarded_data, 0, 1)\n",
    "        result_forwarded_data = F.log_softmax(result_forwarded_data, dim=2)\n",
    "\n",
    "        return result_forwarded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = load_word2vec()\n",
    "token_embeddings_tensor = build_token_embeddings_tensor(token_codes, word2vec, 300)\n",
    "\n",
    "model = NERTagger(unique_symbols_count, unique_labels_count, token_dimension,\n",
    "                  token_embeddings_tensor)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "#TODO: try ADAM, RMSProp - works bad :(\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, weight_decay=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs_count = 100\n",
    "batch_size = 200\n",
    "EPSILON = 1e-10\n",
    "\n",
    "loss_and_metrics_per_epoch = []\n",
    "\n",
    "for epoch in range(epochs_count):\n",
    "\n",
    "    train_data_permutation = np.random.permutation(train_data_size)\n",
    "\n",
    "    data_pass_loss = {}\n",
    "    data_pass_recall = {}\n",
    "    data_pass_precision = {}\n",
    "    data_pass_f1 = {}\n",
    "\n",
    "    for pass_name, pass_size, pass_encoded_symbols, pass_encoded_tokens, pass_encoded_labels in \\\n",
    "            [('train', train_data_size, train_encoded_symbols, train_encoded_tokens, train_encoded_labels),\n",
    "             ('val', val_data_size, val_encoded_symbols, val_encoded_tokens, val_encoded_labels)]:\n",
    "\n",
    "        if pass_name == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        data_pass_loss[pass_name] = 0.0\n",
    "        data_pass_recall[pass_name] = 0.0\n",
    "        data_pass_precision[pass_name] = 0.0\n",
    "        data_pass_f1[pass_name] = 0.0\n",
    "\n",
    "        for i in range(pass_size//batch_size):\n",
    "\n",
    "            batch_indices = slice(batch_size*i, batch_size*(i + 1))\n",
    "\n",
    "            if pass_name == 'train':\n",
    "                model.zero_grad()\n",
    "                batch_indices = train_data_permutation[batch_indices]\n",
    "\n",
    "            batch_encoded_symbols = autograd.Variable(torch.LongTensor(\n",
    "                    pass_encoded_symbols[batch_indices]))\n",
    "\n",
    "            batch_encoded_tokens = pass_encoded_tokens[batch_indices]\n",
    "\n",
    "            if pass_name == 'train':\n",
    "                random_mask = np.random.uniform(size=batch_encoded_tokens.shape)\n",
    "                batch_encoded_tokens[random_mask > 0.7] = token_codes['__unknown__']\n",
    "\n",
    "            batch_encoded_tokens = autograd.Variable(torch.LongTensor(\n",
    "                    batch_encoded_tokens))\n",
    "            batch_encoded_labels_numpy = pass_encoded_labels[batch_indices]\n",
    "            batch_labels_count = batch_encoded_labels_numpy.shape[0]*\\\n",
    "                    batch_encoded_labels_numpy.shape[1]\n",
    "            batch_encoded_labels_numpy = batch_encoded_labels_numpy.ravel()\n",
    "            batch_encoded_labels = autograd.Variable(torch.LongTensor(\n",
    "                    batch_encoded_labels_numpy))\n",
    "\n",
    "            if pass_name == 'train':\n",
    "                predicted_label_probs = model(batch_encoded_symbols, batch_encoded_tokens).view(\n",
    "                        batch_labels_count, unique_labels_count)\n",
    "            else:\n",
    "                #TODO: turn off dropout\n",
    "                predicted_label_probs = model(batch_encoded_symbols, batch_encoded_tokens).view(\n",
    "                        batch_labels_count, unique_labels_count)\n",
    "\n",
    "            predicted_encoded_labels = predicted_label_probs.data.numpy().argmax(axis=1)\n",
    "\n",
    "            relevant_mask = (batch_encoded_labels_numpy != label_codes['none'])\n",
    "            selected_mask = (predicted_encoded_labels != label_codes['none'])\n",
    "            relevant_and_selected_count = np.logical_and(relevant_mask, selected_mask).sum()\n",
    "\n",
    "            recall = relevant_and_selected_count/(relevant_mask.sum() + EPSILON)\n",
    "            precision = relevant_and_selected_count/(selected_mask.sum() + EPSILON)\n",
    "            f1 = 2*recall*precision/(recall + precision + EPSILON)\n",
    "\n",
    "            data_pass_recall[pass_name] += recall\n",
    "            data_pass_precision[pass_name] += precision\n",
    "            data_pass_f1[pass_name] += f1\n",
    "\n",
    "            loss = loss_function(predicted_label_probs, batch_encoded_labels)\n",
    "\n",
    "            if pass_name == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            data_pass_loss[pass_name] += loss.data[0]\n",
    "        \n",
    "        for data_pass_statistic in [data_pass_loss, data_pass_recall,\n",
    "                                    data_pass_precision, data_pass_f1]:\n",
    "            data_pass_statistic[pass_name] /= pass_size//batch_size\n",
    "\n",
    "    print('#{0} epoch:'.format(epoch + 1))\n",
    "    print('\\ttrain_nll={0:.3f} val_nll={1:.3f}'.format(data_pass_loss['train'],\n",
    "                                               data_pass_loss['val']))\n",
    "    print('\\ttrain_recall={0:.3f} val_recall={1:.3f}'.format(data_pass_recall['train'],\n",
    "                                                     data_pass_recall['val']))\n",
    "    print('\\ttrain_precision={0:.3f} val_precision={1:.3f}'.format(data_pass_precision['train'],\n",
    "                                                           data_pass_precision['val']))\n",
    "    print('\\ttrain_f1={0:.3f} val_f1={1:.3f}'.format(data_pass_f1['train'], data_pass_f1['val']))\n",
    "\n",
    "    loss_and_metrics_per_epoch.append((data_pass_loss, data_pass_recall, data_pass_precision,\n",
    "                                       data_pass_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
